{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "IFT-6758 HW-3-(En).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z6yb_gzz1XI"
      },
      "source": [
        "<center><h1> IFT-6758 : Data Science  </h1></center>\n",
        "<center><h2> Fall - 2020 </h2></center> \n",
        "<center><h3> Homework - 3</h3></center> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJcvIo-lWbsB"
      },
      "source": [
        "[Notebook](https://colab.research.google.com/drive/1p4SkPHkEe0IFsvdg1NEXC5i_FloTIeLO) due December 21, 2020 at [23.59 EST](https://www.worldtimebuddy.com/?qm=1&lid=6077243&h=6077243&date=2020-11-06&sln=23-24) as **PDF** on [Gradescope](https://www.gradescope.com/courses/179325/assignments/881881)\n",
        "\n",
        "\n",
        "#**Choose ANY 2 of the 3 questions for answering.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jQvng3cWmbB",
        "cellView": "form"
      },
      "source": [
        "#@title Imports (Run this cell first) { run: \"auto\" }\n",
        "plotting_library = \"matplotlib\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Uncomment this line below if using seaborn\n",
        "#sns.set() \n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "path = 'https://raw.githubusercontent.com/Jhelum-Ch/DataScience_IFT6758/gh-pages/media/{}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCxMAIKgEKbD"
      },
      "source": [
        "# Qn 1 : Natural Language Processing\n",
        "\n",
        "**15 points** = $(3 + 4 + 2 + 4 + 2)$                \n",
        "\n",
        "\n",
        "In this section, you will try to implement functions to obtain the SVD embeddings of words from a text corpus. You will then compare it with Word2Vec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcSrnK6CIazL"
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/Jhelum-Ch/DataScience_IFT6758/gh-pages/media/train.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/Jhelum-Ch/DataScience_IFT6758/gh-pages/media/test.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpAbbl97pGBz"
      },
      "source": [
        "Read lines from text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNfgtvYxD8Mp"
      },
      "source": [
        "def read_text_and_labels(file_name):\n",
        "  file = open(file_name)\n",
        "  lines = file.read().splitlines()\n",
        "  return [tuple(x.split(\"\\t\")) for x in lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4nzawI5pQhT"
      },
      "source": [
        "train_data = read_text_and_labels('train.txt')\n",
        "test_data = read_text_and_labels('test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwMauourEKbF"
      },
      "source": [
        "Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIBqcfptDXUO"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize(data):\n",
        "  return [word_tokenize(item[0].lower()) for item in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Lz4z7Wo-tg"
      },
      "source": [
        "Classification Labels extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77Y6T7_aDXUO"
      },
      "source": [
        "def extract_labels(data):\n",
        "  return [item[-1] for item in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiEHhQVJpCAH"
      },
      "source": [
        "Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VM88-2cDXUO"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "stopwords_english = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY7bdr1rEaYm"
      },
      "source": [
        "def normalize(tokens):\n",
        "  normalized_tokens = []\n",
        "  for token_list in tokens:\n",
        "    normalized = [stemmer.stem(word) for word in token_list if word not in stopwords_english and word not in string.punctuation]\n",
        "    normalized_tokens.append(normalized)\n",
        "  return normalized_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ81VrbQtKiP"
      },
      "source": [
        "train = list(zip(tokenize(train_data),extract_labels(train_data)))\n",
        "test = list(zip(tokenize(test_data),extract_labels(test_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekyPVAbrEKbV"
      },
      "source": [
        "###(a) Write a function that returns the word-document matrix based on the normalized tokens and any other parameters you would require. Note that each data sample (review) in the dataset can be considered as a document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXLa2nwQ2yz5"
      },
      "source": [
        "def make_word_doc_matrix(data):\n",
        "    #YOUR SOLUTION\n",
        "    return word_doc_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb4RR1L72V4B"
      },
      "source": [
        "###(b) Write a function that transforms the review dataset into a dense embedding by representing each review with a dense vector extracted from SVD where dimension = 300. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PodtjZmP2U-k"
      },
      "source": [
        "def svd_embedding(data):\n",
        "    #YOUR SOLUTION\n",
        "    # Call make_word_doc_matrix(data)\n",
        "    return svd_embedding_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm78H3uU2WG_"
      },
      "source": [
        "### (c) Train a Linear SVC model on the dataset using these SVD document embedding representations and report the accuracy on the test-set. \n",
        "\n",
        "### Just add code to compute `svd_train_set` and `svd_test_set` by calling relevant functions and data in the cell below. Then run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HicssPd08_-V"
      },
      "source": [
        "svd_train_set = # YOUR SOLUTION\n",
        "\n",
        "svd_test_set = # YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd55u8OR2VMb"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier = LinearSVC(random_state=0, tol=1e-5)\n",
        "\n",
        "classifier.fit(svd_train_set['X'],svd_train_set['y'])\n",
        "\n",
        "accuracy = classifier.score(svd_test_set['X'],svd_test_set['y'])\n",
        "\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfeB2qgpEKbV"
      },
      "source": [
        "The cell below downloads a minimal pretrained word embeddings model from Google News dataset. The model includes embeddings for 100,000 words and phrases. (It will take a while)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e-095WYrYl9"
      },
      "source": [
        "#Uncomment next line if running locally and not on colab\n",
        "#!pip install gdown\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1kaQdlksJ9jcJJ-q5R3UVjFNb02dYHZVs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpDDa7nawTWs"
      },
      "source": [
        "The model is loaded using gensim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUeEA899EKbW"
      },
      "source": [
        "import gensim.models\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('news-word2vec-100k.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1mBtCbBwXJe"
      },
      "source": [
        "Example of getting the word vector of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhg0Fg-GSg5f"
      },
      "source": [
        "# Example of getting the Word2Vec\n",
        "\n",
        "model.get_vector('happy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei8CNXzmxDCM"
      },
      "source": [
        "Getting (top) similar words from the model. We see that the figures indicate the 'similarity score' or dot product of different word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQT1k3p-w_ts"
      },
      "source": [
        "model.similar_by_word('happy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGUIuAenxT3v"
      },
      "source": [
        "### (d) Identify **FOUR interesting pairs of words** which exist both in the vocabulary of the SVD word embeddings and the Word2Vec embeddings that we have here. Compute the similarity score of the pairs of words in each case (in their respective embedding spaces). \n",
        "\n",
        "###Reason about which of these two embeddings you would use for a sentiment classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skKSLUzdnL-q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-fca8A_0pZ"
      },
      "source": [
        "###(e) Identify any 4 reviews **correctly** classified as positive (with respect to sentiment analysis) by the `classifier` in (c) with the SVD *document embeddings* from the `test_data`. In each of these reviews highlight 1 word *in each case* that you think should have lead to a positive prediction. Verify if these 4 words have any similarity relationships as revealed by their SVD *word embedding* representations.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNhJJxycIiCV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S-3sMO8EKbY"
      },
      "source": [
        "# Qn 2 : Computer Vision \n",
        "\n",
        "**15 points** = $(1.5 + 1 + 1.5 + 1.5 + 1 + 1.5 + 4 + 3)$                \n",
        "\n",
        "In this section, you will add lines of code to transform a sample image using different filters and see their impact on the prediction by a pretrained image recognition model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sNjMK6aIGNo"
      },
      "source": [
        "Import of `cv2`  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIJkhru7GTYS"
      },
      "source": [
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ESWaBjINWo"
      },
      "source": [
        "Download the image file `cat.jpg`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGb94_EMA-4y"
      },
      "source": [
        "!wget 'https://i.ibb.co/SB0c2DW/cat.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyv6SQI6HJoW"
      },
      "source": [
        "###(a) Read the RGB image and store it in a variable `raw_image`. Resize `raw_image` to have dimensions `224 x 224` with the same RGB channels and store this in the same variable `image` and **display** it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FijKl1gqEKbY"
      },
      "source": [
        "raw_image = ## YOUR SOLUTION\n",
        "image = ## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvRM3FlOI_M5"
      },
      "source": [
        "###(b) Convert the `image` into a grayscale image, store it in the variable `grayscale` and display it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux7sv1NRI_WH"
      },
      "source": [
        "grayscale = ## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8isbl7mEKbc"
      },
      "source": [
        "###(c) Consider the image in RGB and generate three images (`r_inverted`, `g_inverted`, `b_inverted`) where in each one the colors of one channel is inverted. **Display** each of the inverted images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo9rFNtGEKbc"
      },
      "source": [
        "r_inverted = ## YOUR SOLUTION\n",
        "\n",
        "g_inverted = ## YOUR SOLUTION\n",
        "\n",
        "b_inverted = ## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50R8BlYJEKbe"
      },
      "source": [
        "###(d) Implement a filter that makes the image 50% lighter and the filter that makes it 50% darker. Make sure to run the cell below to store the results of applying these filters on `image`. \n",
        "\n",
        "###**Display** the resulting images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtn0c8xjEKbe"
      },
      "source": [
        "def lightening_filter(img):\n",
        "    # Make the image 50% darker\n",
        "    # YOUR SOLUTION\n",
        "    return lighter_image\n",
        "    \n",
        "def darkening_filter(img): \n",
        "    # Make the image 50% lighter\n",
        "    # YOUR SOLUTION\n",
        "    return darker_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4-SEfNRKWjL"
      },
      "source": [
        "lighter_image = lightening_filter(image)\n",
        "darker_image = darkening_filter(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuezczIOMp7Q"
      },
      "source": [
        "### (e) Implement a filter that transforms an image into its mirror image about the lateral axis. Run the following cell to apply this filter on `image` and store it as `mirror_filter`. ###\n",
        "\n",
        "###Apply this filter to the image and **display** the output by running the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNM3oVpvMqFV"
      },
      "source": [
        "def mirror_filter(img):\n",
        "    # Transform into mirror image\n",
        "    # YOUR SOLUTION\n",
        "    return mirror_image  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkxaaJ2yfkCj"
      },
      "source": [
        "mirror_image = mirror_filter(image)\n",
        "plt.imshow(mirror_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkVZAMiMEKbg"
      },
      "source": [
        "### (f) Write a function that applies a 3 x  3 mean filter on the image. Mean filter is a simple sliding window that replaces the center pixel value with the average of all pixel values in the window. Note that the border pixels remain unchanged.\n",
        "\n",
        "### Apply this filter to the image and **display** the output by running the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dTF_ybqEKbg"
      },
      "source": [
        "def mean_filter(img):\n",
        "    # Apply 3 x  3 mean filter\n",
        "    # YOUR SOLUTION\n",
        "    return mean_filtered_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiAhhi8DKDJI"
      },
      "source": [
        "mean_filtered_image = mean_filter(image)\n",
        "plt.imshow(mean_filtered_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpHH4Uketki9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ObIFpYKaP4"
      },
      "source": [
        "### (g) Here you are going to make predictions for the 9 images you generated in the previous parts. A Convolutional Neural Network model (VGG-16) pretrained on [ImageNet](http://image-net.org/) dataset has been loaded as `model`. The dataset has 1000 common natural image categories for prediction.\n",
        "\n",
        "### Complete the steps to use these 9 sample images as test set and make predictions of categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgGpy1TmKbf7"
      },
      "source": [
        "test_set = [image, grayscale, r_inverted, g_inverted, b_inverted, mirror_image, lighter_image, darker_image, mean_filtered_image]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocwwMgYkh0D9"
      },
      "source": [
        "test_set_names = ['image', 'grayscale', 'r_inverted', 'g_inverted', 'b_inverted', 'mirror_image', 'lighter_image', 'darker_image', 'mean_filtered_image']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtK2pLd_I-1r",
        "outputId": "626d2a4c-e885-4d50-a4af-b4f9c9c96a19"
      },
      "source": [
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "# Loading the pretrained model with weights from training on ImageNet dataset\n",
        "model = VGG16(weights='imagenet')\n",
        "\n",
        "# TODO : [1] Preprocess the test_set images before you can use the model for prediction \n",
        "# Hint - preprocess_input()\n",
        "# YOUR SOLUTION\n",
        "\n",
        "# TODO : [2] Use the model to obtain the probabilities for all output classes\n",
        "# YOUR SOLUTION\n",
        "\n",
        "# TODO : [3] Decode the predicted class label with highest probability (top-1) and score to display them\n",
        "# Hint - Use decode_predictions(___, top=1)[0][0]\n",
        "# YOUR SOLUTION\n",
        "\n",
        "# TODO : [4] Print this for all 9 samples\n",
        "# YOUR SOLUTION\n",
        "print('Predicted class [{}] : {} with {:.02f}% probability'.format(image_sample, label, score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class [image] : tiger_cat with 79.30% probability\n",
            "Predicted class [grayscale] : ?? with ??% probability\n",
            "...\n",
            "Predicted class [median_filtered_image] : ?? with ??% probability\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcX_vhmjkmXe"
      },
      "source": [
        "### (h) Based on the results you obtained in (g), what can you tell about the impact of the different filters that you implemented on the predictability of the image category by the model? Explain with some intuitions and observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSPkmqIZz1Xk"
      },
      "source": [
        "# Qn 3 : Graph ML \n",
        "\n",
        "**15 points** = $(1.5 + 1.5 + 1.5 + 2.5 + 2.5 + 2.5 + 3)$                \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC3jm4VGXiEv"
      },
      "source": [
        "In this section, you will implement simple functions for obtaining basic graph node embeddings and finally you will observe the differences between them. \n",
        "\n",
        "**Please use only simple mathematical operations (numerical and linear algebra from relevant packages). Use of NetworkX or any other specialized graph library wil not be accepted.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiylnebZLciV"
      },
      "source": [
        "###(a) Write a function `random_adjacency_matrix(n,p)` which returns an adjacency matrix for a \"random graph\" with $n$ vertices where $p$ is the probability of having an edge between any pair of vertices.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeQCdczuz1Xl"
      },
      "source": [
        "def random_adjacency_matrix(n,p):\n",
        "    #YOUR SOLUTION\n",
        "    return A   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx5xxqfdz1Xn"
      },
      "source": [
        "###(b) Write a function `transion_matrix(A)` which, given an adjacency matrix $A$, generates a transition matrix $T$ where probability of each edge (u,v) is calculated as $1/degree(u)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDwHKejWz1Xn"
      },
      "source": [
        "def transition_matrix(A):\n",
        "    #YOUR SOLUTION\n",
        "    return T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iwkaLWyz1Xp"
      },
      "source": [
        "###(c) Write a function `one_hot_embedding(A)` which, given an adjacency matrix A, generates an embedding matrix H where each node is represented with a 1-hot vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK5qonBUz1Xq"
      },
      "source": [
        "def one_hot_embedding(A):\n",
        "    #YOUR SOLUTION\n",
        "    return H"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj327X0Fz1Xr"
      },
      "source": [
        "###(d) Write a function `random_walk_embedding(A,T,H,w,l)` which, given an adjacency matrix $A$, the corresponding transition matrix $T$, and one-hot encoding matrix $H$, performs [random walks](https://en.wikipedia.org/wiki/Random_walk) on the graph from each node $w$ times with length of the walk equal to $l$ and generate an embedding matrix for each node based on the sum of 1-hot encodings of all nodes that are visited during the walks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8uJ5bE_z1Xs"
      },
      "source": [
        "def random_walk_embedding(A,T, H, w, l):\n",
        "    #YOUR SOLUTION\n",
        "    return R"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LmVTdbbz1Xu"
      },
      "source": [
        "###(e) Write a function `k_hop_embedding(A,H,k)` which, given an adjacency matrix $A$, and one-hot node encoding matrix $H$, generates node embedding matrix $kH$ which represents each node as sum of 1-hot encodings of $k$-hop neighbors. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d78ER544z1Xv"
      },
      "source": [
        "def k_hop_embedding(A, H, k):\n",
        "    #YOUR SOLUTION\n",
        "    return kH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTI-7W7mz1Xz"
      },
      "source": [
        "A function `most_similar_nodes(Z)` which, given an node embedding matrix, finds the most similar node for each node in the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu4IdRM-z1Xz"
      },
      "source": [
        "def most_similar_nodes(Z):\n",
        "    # Example : For 5 nodes [1,2,3,4,5], most_similar_node_list could be [5,3,2,1,2] indicating \n",
        "    # 1 -> 5, 2 -> 3, 3 -> 2, 4 -> 1, 5 -> 2\n",
        "    similarity=Z@Z.T\n",
        "    np.fill_diagonal(similarity,0)\n",
        "    most_similar_node_list = similarity.argmax(1) \n",
        "    return most_similar_node_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxScaoM6z1X1"
      },
      "source": [
        "###(f) Generate a random graph where $n=20$, and $p=0.6$, and print the most similar nodes in the graph using the embeddings obtained from :\n",
        "\n",
        "###(Run the correct functions with the right arguments)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgtqzptbz1X1"
      },
      "source": [
        "#@title ⚠️  Random seed : Run this cell mandatorily before running your solution { display-mode: \"form\" }\n",
        "# Do not change or remove\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lecQXyZ6OBX"
      },
      "source": [
        "# Generate the random graph \n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mnooc6Q6KdN"
      },
      "source": [
        "* `random_walk_embedding(l=4, w=10)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dE2MUwuukGq"
      },
      "source": [
        "## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXHefetf6LHa"
      },
      "source": [
        "* `k_hop_embedding(k=1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0Kj45S053pS"
      },
      "source": [
        "## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9JfARPz6LuP"
      },
      "source": [
        "* `k_hop_embedding(k=2)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxs7Gu8u54F5"
      },
      "source": [
        "## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Du1345855D8"
      },
      "source": [
        "###(g) Justify why the most similar nodes obtained in each of the cases in (f) above are different using different node embeddings. Reason intutively about why the obtained 'most similar' nodes are actually similar in each case.  "
      ]
    }
  ]
}